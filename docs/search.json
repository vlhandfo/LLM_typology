[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Untitled Victoria Project",
    "section": "",
    "text": "Abstract\nSubmission for CoNLL 2025.",
    "crumbs": [
      "Abstract"
    ]
  },
  {
    "objectID": "1-introduction/intro.html",
    "href": "1-introduction/intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "In recent years, Transformer-based large language models (LLMs) have become a dominant paradigm in NLP, improving the performance of applications in many language-related tasks. Apart from the general assumption that model performance improves as the amount of training data increases, the precise relationship between the size of the dataset and task-specific performance gains is not well-defined.\nThis study investigates the learning process of monolingual LLMs trained on a set of typologically diverse languages. Specifically, we evaluate intermediate training checkpoints on a set of morphosyntactic tasks to gain insight into the relationship between the amount of training data and the models’ performance on foundational linguistic tasks.\nWhile languages like English display relatively uniform learning curves across all tasks, others show marked task-specific variations, highlighting the importance of taking linguistic aspects into account. We observe variation in ranges of performance gains across languages and identify typological features correlated with the amount of improvement. Likewise, we see that models trained on different languages learn at different rates, and there are typological features correlated to the point at which the model reaches 90% of its overall performance gains, estimated with a best-fit curve.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "2-related_work/related_work.html",
    "href": "2-related_work/related_work.html",
    "title": "2  Related Work",
    "section": "",
    "text": "2.1 Typological Databases",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Related Work</span>"
    ]
  },
  {
    "objectID": "2-related_work/related_work.html#sec-databases",
    "href": "2-related_work/related_work.html#sec-databases",
    "title": "2  Related Work",
    "section": "",
    "text": "2.1.1 WALS\nDryer and Haspelmath (2013)\n The World Atlas of Language Structures (WALS) is a collection of linguistic features documenting the structural elements of many of the world’s languages. This database contains data on a variety of properties including, but not limited to, phonology, grammar, lexicon, language family, and the general location of where the language is most spoken. Since languages are grouped into categories based on what they predominately exhibit, it is a useful resource for exploring the typological features of languages and cross-linguistic patterns (Dryer and Haspelmath (2013)).\n\n\n2.1.2 Grambank\nSkirgård et al. (2023)\nGrambank is another database that compiles various grammatical features for cross-linguistic analyses. Most of the features are structured as questions where the corresponding feature values are coded as 0 if the answer is no and 1 if yes. This database and its encodings are the foundation for vector representations of the languages.\n\n\n\n\nDryer, Matthew S., and Martin Haspelmath, eds. 2013. WALS Online (V2020.3). Data set. Zenodo. https://doi.org/10.5281/zenodo.7385533.\n\n\nSkirgård, Hedvig, Hannah J. Haynie, Harald Hammarström, Damián E. Blasi, Jeremy Collins, Jay Latarche, Jakob Lesage, et al. 2023. “Grambank V1.0.” Zenodo. https://doi.org/10.5281/zenodo.7740140.\n\n\nZhang, Yian, Alex Warstadt, Xiaocheng Li, and Samuel R. Bowman. 2021. “When Do You Need Billions of Words of Pretraining Data?” In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 1112–25. Online: Association for Computational Linguistics. https://doi.org/10.18653/v1/2021.acl-long.90.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Related Work</span>"
    ]
  },
  {
    "objectID": "3-models_data/models_data.html",
    "href": "3-models_data/models_data.html",
    "title": "3  Models and Data",
    "section": "",
    "text": "3.1 The HPLT Project\nThe High-Performance Language Technology (HPLT) project seeks to combine large amounts of data and tools for high-performance computing to produce and make high-quality language models available Gibert et al. (2024). It provides large datasets for a variety of languages and base models trained on them. These datasets and models are the foundation of our experiments.\nTODO: write about them pretraining the models and providing intermediate checkpoints",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models and Data</span>"
    ]
  },
  {
    "objectID": "3-models_data/models_data.html#evaluation-data",
    "href": "3-models_data/models_data.html#evaluation-data",
    "title": "3  Models and Data",
    "section": "3.2 Evaluation Data",
    "text": "3.2 Evaluation Data\n\nTo evaluate each language model, we use the Universal Dependencies 2.14 treebanks (UD; Nivre et al. (2017), (2020)). The treebanks are formatted in the same way which allows for simpler comparison of the results of the cross-linguistic experiments. Likewise, as the treebanks include text from different genres, the data covers a broad range of language use.\nThe treebanks for the selected languages vary in size, both in terms of number of words and sentences. To ensure the comparability of the results, subsets of each partition of the treebanks were shuffled and normalized by the number of word forms. While the number of sentences in the result subsets still varies, this step makes sure that each model encounters roughly the same number of tokens during the fine-tuning and evaluation steps.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models and Data</span>"
    ]
  },
  {
    "objectID": "3-models_data/models_data.html#language-and-feature-selection",
    "href": "3-models_data/models_data.html#language-and-feature-selection",
    "title": "3  Models and Data",
    "section": "3.3 Language and Feature Selection",
    "text": "3.3 Language and Feature Selection\nThe languages for this study were chosen based on two criteria: the availability of resources and linguistic diversity. Available resources include the size of the dataset from the HPLT project and the treebanks from Universal Dependencies, as well as the typological features available in the databases (see Section 2.1). Linguistic diversity was determined by examining typological, genealogical, and geographic differences available in WALS, specifically the WALS 100-sample.\nThe features were selected from WALS and Grambank, and those from the WALS database were manually coverted to the Question: Binary Encoding format of the Grambank features (e.g., “81A: Order of Subject, Object and Verb” from WALS became WALS81A: Is the dominant word order in a transitive clause SVO?)\nAfter identifying relevant morphosyntactic features, the set was filtered, removing features with no variance and/or missing values for any selected language. Then, each feature was categorized into a group according to the type of feature it is related to. The largest group, marking, relate to grammatical phenomena which morphologically indicated on words. Order are features related to aspects of word order and syntax. Features in the group formation represent different methods for creating words and clauses. The groups determiners and lexicon correspond to the types of determiners and words, respectively. Lastly, case pertains to morphological marking of cases. A complete list of selected features is shown in Table 3.1.\nWith the features and language selected, it was straight-forward to combine all of the values to create a binary vector representation for each language. Figure 3.1 shows the PCA of the languages according to these vector representations.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.1: PCA of the selected languages according to their feature values.\n\n\n\n\n3.3.1 Selected Features\n\n\n\n\n\n\n\n    \n      \n      Feature\n      Name\n      Group\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.2.1 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\n\n\nTable 3.1: An overview of all selected features and assigned group. The prefix GB indicates a feature from Grambank, and WALS is the adapted feature from the WALS database.\n\n\n\n\n\n\n\nGibert, Ona de, Graeme Nail, Nikolay Arefyev, Marta Bañón, Jelmer van der Linde, Shaoxiong Ji, Jaume Zaragoza-Bernabeu, et al. 2024. “A New Massive Multilingual Dataset for High-Performance Language Technologies.” In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), edited by Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue, 1116–28. Torino, Italia: ELRA; ICCL. https://aclanthology.org/2024.lrec-main.100.\n\n\nNivre, Joakim, Marie-Catherine de Marneffe, Filip Ginter, Jan Hajič, Christopher D. Manning, Sampo Pyysalo, Sebastian Schuster, Francis Tyers, and Daniel Zeman. 2020. “Universal Dependencies V2: An Evergrowing Multilingual Treebank Collection.” In Proceedings of the Twelfth Language Resources and Evaluation Conference, edited by Nicoletta Calzolari, Frédéric Béchet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, et al., 4034–43. Marseille, France: European Language Resources Association. https://aclanthology.org/2020.lrec-1.497/.\n\n\nNivre, Joakim, Daniel Zeman, Filip Ginter, and Francis Tyers. 2017. “Universal Dependencies.” In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Tutorial Abstracts, edited by Alexandre Klementiev and Lucia Specia. Valencia, Spain: Association for Computational Linguistics. https://aclanthology.org/E17-5001/.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models and Data</span>"
    ]
  },
  {
    "objectID": "4-methods/methods.html",
    "href": "4-methods/methods.html",
    "title": "4  Methods",
    "section": "",
    "text": "4.1 Evaluation",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "4-methods/methods.html#evaluation",
    "href": "4-methods/methods.html#evaluation",
    "title": "4  Methods",
    "section": "",
    "text": "Uses the evaluation script from CoNLL 2018 Shared Task\nMaybe don’t need to include all metrics in this paper (?): Considers on Lemmas, UPOS, UAS, LAS, CLAS, MLAS, BLEX",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "4-methods/methods.html#analysis",
    "href": "4-methods/methods.html#analysis",
    "title": "4  Methods",
    "section": "4.2 Analysis",
    "text": "4.2 Analysis\nFollowing the procedure in Zhang et al. (2021), we compare the relative results of the performance of each checkpoint.\n\nget statistics:\n\ndelta: the amount of improvement\nP90: the estimated point where the model reaches 90 percent of it’s overall performance gains.\n\nmax-min normalization -&gt; best fit curve.\n\n\n\n\n\n\n\nZhang, Yian, Alex Warstadt, Xiaocheng Li, and Samuel R. Bowman. 2021. “When Do You Need Billions of Words of Pretraining Data?” In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 1112–25. Online: Association for Computational Linguistics. https://doi.org/10.18653/v1/2021.acl-long.90.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "4-methods/methods.html#footnotes",
    "href": "4-methods/methods.html#footnotes",
    "title": "4  Methods",
    "section": "",
    "text": "HPLT Tools: https://github.com/hplt-project/HPLT-WP4↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "5-results/results.html",
    "href": "5-results/results.html",
    "title": "5  Results",
    "section": "",
    "text": "5.1 Learning Curve Variation",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "5-results/results.html#learning-curve-variation",
    "href": "5-results/results.html#learning-curve-variation",
    "title": "5  Results",
    "section": "",
    "text": "5.1.1 Variation of a Model\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.1: While the learning curves for English model are rather homogenous, it is not the case for the Japanese model. The learning curves for the Chinese models on UAS and Lemmas are similar to the English model, however, all checkpoints of the Chinese model achieve \\(&gt; 99\\%\\) accuracy on lemmatization.\n\n\n\n\n\n5.1.2 Variation on the Same Task\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.2: Given a task, different models may learn at different rates than others.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "5-results/results.html#delta-variation",
    "href": "5-results/results.html#delta-variation",
    "title": "5  Results",
    "section": "5.2 Delta Variation",
    "text": "5.2 Delta Variation\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.3: The amount of improvement (delta) varies across different tasks and models.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "5-results/results.html#typological-correlations",
    "href": "5-results/results.html#typological-correlations",
    "title": "5  Results",
    "section": "5.3 Typological Correlations",
    "text": "5.3 Typological Correlations\n\n\n\nTable 5.1: Feature correlations with P90 and pvalue &lt;0.01.\n\n\n\n\n\n\n    \n      \n      Name\n      Task\n      Correlation\n    \n    \n      Feature\n      \n      \n      \n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.2.1 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 5.2: Feature correlations with Delta and pvalue &lt;0.01.\n\n\n\n\n\n\n    \n      \n      Name\n      Task\n      Correlation\n    \n    \n      Feature\n      \n      \n      \n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.2.1 from the internet...\n(need help?)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "7-conclusion/conclusion.html",
    "href": "7-conclusion/conclusion.html",
    "title": "7  Conclusion",
    "section": "",
    "text": "Work in Progress",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Dryer, Matthew S., and Martin Haspelmath, eds. 2013. WALS Online\n(V2020.3). Data set. Zenodo. https://doi.org/10.5281/zenodo.7385533.\n\n\nGibert, Ona de, Graeme Nail, Nikolay Arefyev, Marta Bañón, Jelmer van\nder Linde, Shaoxiong Ji, Jaume Zaragoza-Bernabeu, et al. 2024. “A\nNew Massive Multilingual Dataset for High-Performance Language\nTechnologies.” In Proceedings of the 2024 Joint International\nConference on Computational Linguistics, Language Resources and\nEvaluation (LREC-COLING 2024), edited by Nicoletta Calzolari,\nMin-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and\nNianwen Xue, 1116–28. Torino, Italia: ELRA; ICCL. https://aclanthology.org/2024.lrec-main.100.\n\n\nNivre, Joakim, Marie-Catherine de Marneffe, Filip Ginter, Jan Hajič,\nChristopher D. Manning, Sampo Pyysalo, Sebastian Schuster, Francis\nTyers, and Daniel Zeman. 2020. “Universal\nDependencies V2: An Evergrowing Multilingual Treebank\nCollection.” In Proceedings of the Twelfth Language Resources\nand Evaluation Conference, edited by Nicoletta Calzolari, Frédéric\nBéchet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry\nDeclerck, Sara Goggi, et al., 4034–43. Marseille, France: European\nLanguage Resources Association. https://aclanthology.org/2020.lrec-1.497/.\n\n\nNivre, Joakim, Daniel Zeman, Filip Ginter, and Francis Tyers. 2017.\n“Universal Dependencies.” In\nProceedings of the 15th Conference of the European\nChapter of the Association for Computational Linguistics: Tutorial\nAbstracts, edited by Alexandre Klementiev and Lucia Specia.\nValencia, Spain: Association for Computational Linguistics. https://aclanthology.org/E17-5001/.\n\n\nSkirgård, Hedvig, Hannah J. Haynie, Harald Hammarström, Damián E. Blasi,\nJeremy Collins, Jay Latarche, Jakob Lesage, et al. 2023. “Grambank\nV1.0.” Zenodo. https://doi.org/10.5281/zenodo.7740140.\n\n\nZhang, Yian, Alex Warstadt, Xiaocheng Li, and Samuel R. Bowman. 2021.\n“When Do You Need Billions of Words of Pretraining Data?”\nIn Proceedings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Conference on\nNatural Language Processing (Volume 1: Long Papers), 1112–25.\nOnline: Association for Computational Linguistics. https://doi.org/10.18653/v1/2021.acl-long.90.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "appendix/appendix.html",
    "href": "appendix/appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "All Features\nFeature\n      Name\n      Group\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.2.1 from the internet...\n(need help?)",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "appendix/appendix.html#raw-p90-thresholds",
    "href": "appendix/appendix.html#raw-p90-thresholds",
    "title": "Appendix",
    "section": "Raw P90 Thresholds",
    "text": "Raw P90 Thresholds\n\n\n\n\n    \n      \n      Lemmas\n      UPOS\n      UAS\n      LAS\n      CLAS\n      MLAS\n      BLEX\n    \n    \n      Language\n      \n      \n      \n      \n      \n      \n      \n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.2.1 from the internet...\n(need help?)",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "appendix/appendix.html#raw-delta-values",
    "href": "appendix/appendix.html#raw-delta-values",
    "title": "Appendix",
    "section": "Raw Delta Values",
    "text": "Raw Delta Values\n\n\n\n\n    \n      \n      Lemmas\n      UPOS\n      UAS\n      LAS\n      CLAS\n      MLAS\n      BLEX\n    \n    \n      Language\n      \n      \n      \n      \n      \n      \n      \n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.2.1 from the internet...\n(need help?)",
    "crumbs": [
      "Appendix"
    ]
  }
]