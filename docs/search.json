[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Untitled Victoria Project",
    "section": "",
    "text": "Abstract\nSubmission for CoNLL 2025.",
    "crumbs": [
      "Abstract"
    ]
  },
  {
    "objectID": "1-introduction/intro.html",
    "href": "1-introduction/intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "In recent years, Transformer-based large language models (LLMs) have become a dominant paradigm in NLP, improving the performance of applications in many language-related tasks. Apart from the general assumption that model performance improves as the amount of training data increases, the precise relationship between the size of the dataset and task-specific performance gains is not well-defined.\nThis study investigates the learning process of monolingual LLMs trained on a set of typologically diverse languages. Specifically, we evaluate intermediate training checkpoints on a set of morphosyntactic tasks to gain insight into the relationship between the amount of training data and the models’ performance on foundational linguistic tasks.\nWhile languages like English display relatively uniform learning curves across all tasks, others show marked task-specific variations, highlighting the importance of taking linguistic aspects into account. We observe variation in ranges of performance gains across languages and identify typological features correlated with the amount of improvement. Likewise, we see that models trained on different languages learn at different rates, and there are typological features correlated to the point at which the model reaches 90% of its overall performance gains, estimated with a best-fit curve.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "2-related_work/related_work.html",
    "href": "2-related_work/related_work.html",
    "title": "2  Related Work",
    "section": "",
    "text": "2.1 Typological Databases",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Related Work</span>"
    ]
  },
  {
    "objectID": "2-related_work/related_work.html#sec-databases",
    "href": "2-related_work/related_work.html#sec-databases",
    "title": "2  Related Work",
    "section": "",
    "text": "2.1.1 WALS\nDryer and Haspelmath (2013)\n The World Atlas of Language Structures (WALS) is a collection of linguistic features documenting the structural elements of many of the world’s languages. This database contains data on a variety of properties including, but not limited to, phonology, grammar, lexicon, language family, and the general location of where the language is most spoken. Since languages are grouped into categories based on what they predominately exhibit, it is a useful resource for exploring the typological features of languages and cross-linguistic patterns (Dryer and Haspelmath (2013)).\n\n\n2.1.2 Grambank\nSkirgård et al. (2023)\nGrambank is another database that compiles various grammatical features for cross-linguistic analyses. Most of the features are structured as questions where the corresponding feature values are coded as 0 if the answer is no and 1 if yes. This database and its encodings are the foundation for vector representations of the languages.\n\n\n\n\nDryer, Matthew S., and Martin Haspelmath, eds. 2013. WALS Online (V2020.3). Data set. Zenodo. https://doi.org/10.5281/zenodo.7385533.\n\n\nSkirgård, Hedvig, Hannah J. Haynie, Harald Hammarström, Damián E. Blasi, Jeremy Collins, Jay Latarche, Jakob Lesage, et al. 2023. “Grambank V1.0.” Zenodo. https://doi.org/10.5281/zenodo.7740140.\n\n\nZhang, Yian, Alex Warstadt, Xiaocheng Li, and Samuel R. Bowman. 2021. “When Do You Need Billions of Words of Pretraining Data?” In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 1112–25. Online: Association for Computational Linguistics. https://doi.org/10.18653/v1/2021.acl-long.90.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Related Work</span>"
    ]
  },
  {
    "objectID": "3-models_data/models_data.html",
    "href": "3-models_data/models_data.html",
    "title": "3  Models and Data",
    "section": "",
    "text": "3.1 The HPLT Project\nThe High-Performance Language Technology (HPLT) project seeks to combine large amounts of data and tools for high-performance computing to produce and make high-quality language models available Gibert et al. (2024). It provides large datasets for a variety of languages and base models trained on them. These datasets and models are the foundation of our experiments.\nTODO: write about them pretraining the models and providing intermediate checkpoints",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models and Data</span>"
    ]
  },
  {
    "objectID": "3-models_data/models_data.html#evaluation-data",
    "href": "3-models_data/models_data.html#evaluation-data",
    "title": "3  Models and Data",
    "section": "3.2 Evaluation Data",
    "text": "3.2 Evaluation Data\n\nTo evaluate each language model, we use the Universal Dependencies 2.14 treebanks (UD; Nivre et al. (2017), (2020)). The treebanks are formatted in the same way, which allows for a simpler comparison of the results of the cross-linguistic experiments. Likewise, as the treebanks include text from different genres, the data covers a broad range of language use.\nThe treebanks for the selected languages vary in size, both in terms of the number of words and sentences. To ensure the comparability of the results, subsets of each partition of the treebanks were shuffled and normalized by the number of word forms. While the number of sentences in the result subsets still varies, this step ensures that each model encounters roughly the same number of tokens during the fine-tuning and evaluation steps.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models and Data</span>"
    ]
  },
  {
    "objectID": "3-models_data/models_data.html#language-and-feature-selection",
    "href": "3-models_data/models_data.html#language-and-feature-selection",
    "title": "3  Models and Data",
    "section": "3.3 Language and Feature Selection",
    "text": "3.3 Language and Feature Selection\nThe languages for this study were chosen based on two criteria: the availability of resources and linguistic diversity. Available resources include the size of the dataset from the HPLT project and the treebanks from Universal Dependencies, as well as the typological features available in the databases (see Section 2.1). Linguistic diversity was determined by examining typological, genealogical, and geographic differences available in WALS, specifically the WALS 100-sample.\nThe features were selected from WALS and Grambank, and those from the WALS database were manually coverted to the Question: Binary Encoding format of the Grambank features (e.g., “81A: Order of Subject, Object and Verb” from WALS became WALS81A: Is the dominant word order in a transitive clause SVO?)\nAfter identifying relevant morphosyntactic features, the set was filtered, removing features with no variance and/or missing values for any selected language. Then, each feature was categorized into a group according to the type of feature it is related to. The largest group, marking, relates to grammatical phenomena which are morphologically indicated on words. Order are features related to aspects of word order and syntax. Features in the group formation represent different methods for creating words and clauses. The groups determiners and lexicon correspond to the types of determiners and words, respectively. Lastly, case pertains to the morphological marking of cases. A complete list of selected features is shown in Table 3.1.\nWith the features and language selected, it was straightforward to combine all of the values to create a binary vector representation for each language. Figure 3.1 shows the PCA of the languages according to these vector representations.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.1: PCA of the selected languages according to their feature values.\n\n\n\n\n3.3.1 Selected Features\n\n\n\n\n\n\n\n    \n      \n      Feature\n      Name\n      Group\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.2.1 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\n\n\nTable 3.1: An overview of all selected features and assigned group. The prefix GB indicates a feature from Grambank, and WALS is the adapted feature from the WALS database.\n\n\n\n\n\n\n\nGibert, Ona de, Graeme Nail, Nikolay Arefyev, Marta Bañón, Jelmer van der Linde, Shaoxiong Ji, Jaume Zaragoza-Bernabeu, et al. 2024. “A New Massive Multilingual Dataset for High-Performance Language Technologies.” In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), edited by Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue, 1116–28. Torino, Italia: ELRA; ICCL. https://aclanthology.org/2024.lrec-main.100.\n\n\nNivre, Joakim, Marie-Catherine de Marneffe, Filip Ginter, Jan Hajič, Christopher D. Manning, Sampo Pyysalo, Sebastian Schuster, Francis Tyers, and Daniel Zeman. 2020. “Universal Dependencies V2: An Evergrowing Multilingual Treebank Collection.” In Proceedings of the Twelfth Language Resources and Evaluation Conference, edited by Nicoletta Calzolari, Frédéric Béchet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, et al., 4034–43. Marseille, France: European Language Resources Association. https://aclanthology.org/2020.lrec-1.497/.\n\n\nNivre, Joakim, Daniel Zeman, Filip Ginter, and Francis Tyers. 2017. “Universal Dependencies.” In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Tutorial Abstracts, edited by Alexandre Klementiev and Lucia Specia. Valencia, Spain: Association for Computational Linguistics. https://aclanthology.org/E17-5001/.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models and Data</span>"
    ]
  },
  {
    "objectID": "4-methods/methods.html",
    "href": "4-methods/methods.html",
    "title": "4  Methods",
    "section": "",
    "text": "4.1 Evaluation",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "4-methods/methods.html#evaluation",
    "href": "4-methods/methods.html#evaluation",
    "title": "4  Methods",
    "section": "",
    "text": "Uses the evaluation script from CoNLL 2018 Shared Task\nMaybe don’t need to include all metrics in this paper (?): Considers on Lemmas, UPOS, UAS, LAS, CLAS, MLAS, BLEX",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "4-methods/methods.html#analysis",
    "href": "4-methods/methods.html#analysis",
    "title": "4  Methods",
    "section": "4.2 Analysis",
    "text": "4.2 Analysis\nFollowing the procedure in Zhang et al. (2021), we compare the relative results of the performance of each checkpoint.\n\nget statistics:\n\ndelta: the amount of improvement\nP90: the estimated point where the model reaches 90 percent of it’s overall performance gains.\n\nmax-min normalization -&gt; best fit curve.\n\n\n\n\n\n\n\nZhang, Yian, Alex Warstadt, Xiaocheng Li, and Samuel R. Bowman. 2021. “When Do You Need Billions of Words of Pretraining Data?” In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 1112–25. Online: Association for Computational Linguistics. https://doi.org/10.18653/v1/2021.acl-long.90.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "4-methods/methods.html#footnotes",
    "href": "4-methods/methods.html#footnotes",
    "title": "4  Methods",
    "section": "",
    "text": "HPLT Tools: https://github.com/hplt-project/HPLT-WP4↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "5-results/results.html",
    "href": "5-results/results.html",
    "title": "5  Results",
    "section": "",
    "text": "5.1 Learning Curve Variation",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "5-results/results.html#learning-curve-variation",
    "href": "5-results/results.html#learning-curve-variation",
    "title": "5  Results",
    "section": "",
    "text": "Question: When there is little over all improvement, do these curves actually tell us anything?\n\n\n\n\n\nThis point may not be as straight forward as I orginially thought. The main example I saw was for the Japanese model, however, the tokenization strategy might play a bigger role than I thought.\n\n\n\n\n\n\n\n\nHowever, we see that the Indonesian model has the same trend on the dependency parsing tasks, so there is a bit of variation there.\n\n\n\n\n5.1.1 Variation of a Model\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.1: There is some variation between different tasks for different models, but it seems less so than I originally thought.\n\n\n\n\n\nShow Tables with Raw Values\n\n\n\n\nTable 5.1: Raw values for lemmatization.\n\n\n\n\n\n\n    \n      Step\n      Language\n      P90\n      0\n      30\n      300\n      3050\n      31250\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.2.1 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 5.2: Raw values for UPOS.\n\n\n\n\n\n\n    \n      Step\n      Language\n      P90\n      0\n      30\n      300\n      3050\n      31250\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.2.1 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 5.3: Raw values for UAS.\n\n\n\n\n\n\n    \n      Step\n      Language\n      P90\n      0\n      30\n      300\n      3050\n      31250\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.2.1 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.1.2 Variation on the Same Task\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.2: Given a task, different models may learn at different rates than others.\n\n\n\n\n5.1.2.1 Lemmatization\nSee learning curves in Figure 5.2 and raw plot in Figure 5.3.\nObservations:\n(Obvious) Analytic languages (Chinese and Vietnamese) have the lowest value of P90, the smallest range of improvement, and the highest overall accuracy scores on lemmatization.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.3: Raw lemmatization accuracy for all languages\n\n\n\n\n\n5.1.2.2 UPOS\nThe curves for UPOS are more homogenous than some of the other tasks, however, there are correlations with marking features and P90 (see Section 5.3).\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.4: Raw UPOS accuracy for all languages\n\n\n\n\n\n5.1.2.3 Dependency Parsing\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.5: Raw UAS accuracy for all languages\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.6: Raw UAS accuracy for fusional languages\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUAS with and without Japanese",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "5-results/results.html#delta-variation",
    "href": "5-results/results.html#delta-variation",
    "title": "5  Results",
    "section": "5.2 Delta Variation",
    "text": "5.2 Delta Variation\nThe amount of overall improvement (delta) varies across languages and tasks.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.7: The amount of improvement (delta) varies across different tasks and models.\n\n\n\nAgglutination is correlated with the amount of overall improvement.\nUPDATE: while there is a visual difference, the correlation is no longer significant.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 5.4: The correlation and pvalue between agglutinative languages and delta value. None of the correlations are statistically significant.\n\n\n\n\n\n\n    \n      \n      Correlation\n      Pvalue\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.2.1 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\n\n\n\nFusional languages are also correlated with overall improvement on lemmatization.\n\n\n\n\n\n\n\n\n\n\n\nTable 5.5: The correlation and pvalue between fusional languages and delta value.\n\n\n\n\n\n\n    \n      \n      Correlation\n      Pvalue\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.2.1 from the internet...\n(need help?)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "5-results/results.html#sec-typological-correlations",
    "href": "5-results/results.html#sec-typological-correlations",
    "title": "5  Results",
    "section": "5.3 Typological Correlations",
    "text": "5.3 Typological Correlations\n\n\n\n\n\n\n\n    \n      \n      Feature\n      Name\n      Group\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.2.1 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\n\n\nTable 5.6: The selected features from Grambank and WALS.\n\n\n\n\n5.3.1 Correlations with P90\n\n\n\n\n\n\nNote\n\n\n\nRemember, P90 is better when lower. That means that the estimated point of 90 percent of all improvement occurs earlier in the learning process.\n\n\nThese correlations include all 11 languages and all 56 features individually. There is an additional feature, {GROUP}_sum, which is a count of the number of features present in the given group.\n\n\n\n\n\n\n\n    \n      \n      Task\n      Group\n      Feature\n      Correlation\n      PValue\n      Name\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.2.1 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\n\n\nTable 5.7: The features with correlations with P90 and pvalue &lt; 0.05.\n\n\n\n\n5.3.1.1 Low variance\n\n\n\n\n\n\nNote\n\n\n\nLow variance features are those that have &lt;2 languages that are different than the rest.\n\n\n\n\n\nTable 5.8: The values of the features correlated with P90.\n\n\n\n\n\nLow variance features: ['GB047', 'GB049', 'GB092', 'GB093']\n\n\n\n\n    \n      Feature\n      GB044\n      GB047\n      GB048\n      GB049\n      GB091\n      GB092\n      GB093\n      GB122\n      WALS92A\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.2.1 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.3.1.2 Correlations with UPOS-P90\nFeature GB044: Is there productive morphological plural marking on nouns?\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.8: There is a clear division for those languages with the feature GB044=1 and those without. Note Here, we can debate that Indonesian should be encoded as a 1.\n\n\n\n\n\n\n5.3.2 Correlations with Delta\n\n\n\n\n\n\n\n    \n      \n      Task\n      Group\n      Feature\n      Correlation\n      PValue\n      Name\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.2.1 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\n\n\nTable 5.9: The features with correlations with delta and pvalue &lt; 0.05.\n\n\n\n\n5.3.2.1 Low variance\n\n\n\n\n\n\nNote\n\n\n\nLow variance features are those that have &lt;2 languages that are different than the rest.\n\n\n\n\n\nTable 5.10: The values of the correlated features.\n\n\n\n\n\nLow variance features: ['GB042', 'GB043', 'GB090', 'GB094']\n\n\n\n\n    \n      Feature\n      GB035\n      GB042\n      GB043\n      GB044\n      GB057\n      GB082\n      GB084\n      GB089\n      GB090\n      GB091\n      GB094\n      GB107\n      GB132\n      WALS144A\n      WALS81A\n      WALS83A\n      WALS93A\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.2.1 from the internet...\n(need help?)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "7-conclusion/conclusion.html",
    "href": "7-conclusion/conclusion.html",
    "title": "7  Conclusion",
    "section": "",
    "text": "Work in Progress",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Dryer, Matthew S., and Martin Haspelmath, eds. 2013. WALS Online\n(V2020.3). Data set. Zenodo. https://doi.org/10.5281/zenodo.7385533.\n\n\nGibert, Ona de, Graeme Nail, Nikolay Arefyev, Marta Bañón, Jelmer van\nder Linde, Shaoxiong Ji, Jaume Zaragoza-Bernabeu, et al. 2024. “A\nNew Massive Multilingual Dataset for High-Performance Language\nTechnologies.” In Proceedings of the 2024 Joint International\nConference on Computational Linguistics, Language Resources and\nEvaluation (LREC-COLING 2024), edited by Nicoletta Calzolari,\nMin-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and\nNianwen Xue, 1116–28. Torino, Italia: ELRA; ICCL. https://aclanthology.org/2024.lrec-main.100.\n\n\nNivre, Joakim, Marie-Catherine de Marneffe, Filip Ginter, Jan Hajič,\nChristopher D. Manning, Sampo Pyysalo, Sebastian Schuster, Francis\nTyers, and Daniel Zeman. 2020. “Universal\nDependencies V2: An Evergrowing Multilingual Treebank\nCollection.” In Proceedings of the Twelfth Language Resources\nand Evaluation Conference, edited by Nicoletta Calzolari, Frédéric\nBéchet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry\nDeclerck, Sara Goggi, et al., 4034–43. Marseille, France: European\nLanguage Resources Association. https://aclanthology.org/2020.lrec-1.497/.\n\n\nNivre, Joakim, Daniel Zeman, Filip Ginter, and Francis Tyers. 2017.\n“Universal Dependencies.” In\nProceedings of the 15th Conference of the European\nChapter of the Association for Computational Linguistics: Tutorial\nAbstracts, edited by Alexandre Klementiev and Lucia Specia.\nValencia, Spain: Association for Computational Linguistics. https://aclanthology.org/E17-5001/.\n\n\nSkirgård, Hedvig, Hannah J. Haynie, Harald Hammarström, Damián E. Blasi,\nJeremy Collins, Jay Latarche, Jakob Lesage, et al. 2023. “Grambank\nV1.0.” Zenodo. https://doi.org/10.5281/zenodo.7740140.\n\n\nZhang, Yian, Alex Warstadt, Xiaocheng Li, and Samuel R. Bowman. 2021.\n“When Do You Need Billions of Words of Pretraining Data?”\nIn Proceedings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Conference on\nNatural Language Processing (Volume 1: Long Papers), 1112–25.\nOnline: Association for Computational Linguistics. https://doi.org/10.18653/v1/2021.acl-long.90.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "appendix/appendix.html",
    "href": "appendix/appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "All Features\nFeature\n      Name\n      Group\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.2.1 from the internet...\n(need help?)",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "appendix/appendix.html#raw-p90-thresholds",
    "href": "appendix/appendix.html#raw-p90-thresholds",
    "title": "Appendix",
    "section": "Raw P90 Thresholds",
    "text": "Raw P90 Thresholds\n\n\n\n\n    \n      \n      Lemmas\n      UPOS\n      UAS\n      LAS\n      CLAS\n      MLAS\n      BLEX\n    \n    \n      Language\n      \n      \n      \n      \n      \n      \n      \n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.2.1 from the internet...\n(need help?)",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "appendix/appendix.html#raw-delta-values",
    "href": "appendix/appendix.html#raw-delta-values",
    "title": "Appendix",
    "section": "Raw Delta Values",
    "text": "Raw Delta Values\n\n\n\n\n    \n      \n      Lemmas\n      UPOS\n      UAS\n      LAS\n      CLAS\n      MLAS\n      BLEX\n    \n    \n      Language\n      \n      \n      \n      \n      \n      \n      \n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.2.1 from the internet...\n(need help?)",
    "crumbs": [
      "Appendix"
    ]
  }
]